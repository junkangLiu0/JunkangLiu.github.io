
---
layout: home
title: 刘俊康 | Junkang Liu
subtitle: 博士研究生 @ 天津大学 <br> 研究方向：联邦学习 | 优化算法 | 大模型高效微调
---

<div style="text-align: center;">
  <img src="/fig1.jpg" alt="avatar" width="160" style="border-radius: 50%; box-shadow: 0 0 10px rgba(0,0,0,0.15); margin-bottom: 20px;">

  
  <div style="margin: 10px 0;">
    <a class="btn btn--primary" href="/assets/cv/cv_liu_junkang.pdf" target="_blank">📄 下载简历 / Download CV</a>
    <a class="btn btn--inverse" href="mailto:junkangliukk@gmail.com">✉️ 联系我 / Email</a>
  </div>

  <div style="font-size: 1.2rem;">
    <a href="https://github.com/junkangliu" target="_blank">🌐 GitHub</a> |
    <a href="https://scholar.google.com" target="_blank">📚 Google Scholar</a>
  </div>
</div>

---

🎓 **简介 / About Me**

我目前是 **天津大学** 计算机科学与技术专业博士研究生，硕士毕业于 **西安电子科技大学人工智能学院**。研究兴趣涵盖：

- ⚙️ 联邦学习（Federated Learning）
- 🔐 差分隐私优化与泛化理论
- 🧠 参数高效微调（PEFT, LoRA, BitFit 等）
- 📉 非凸优化理论

曾以第一作者身份在 **ICML、ACM MM 等 CCF A 类会议** 发表多篇论文。

---

📫 **联系方式 / Contact**

- 📧 邮箱：junkangliukk@gmail.com  
- 🌐 GitHub：[@junkangliu](https://github.com/junkangliu)  
- 📚 Google Scholar：[链接](https://scholar.google.com)
  


📰 **最新动态 / News**
- ✉️ [2025] 向 AAAI 投稿最新成果《FedAdamW: Convergence and Generalization in FL》
- ✉️ [2025] 向 TIFS 投稿最新成果《Finding Globally Flat Minima for DP Federated Learning》
- ✉️ [2025] 向 TIFS 投稿最新成果《Finding Globally Flat Minima for DP Federated Learning》
- ✅ [2025] ACM MM 论文《Generalized Federated Learning via Sharpness Aware Minimization with Nesterov Extrapolation》被接收！
- ✅ [2025] ICML 论文《Improving Generalization in Federated Learning with Heterogeneous Data via Momentum-Based SWA》被接收！
- 🏆 [2024] 荣获研究生国家奖学金
- ✅ [2025] ACM MM 论文《FedBCGD: Communication-Efficient Accelerated Block Coordinate Gradient Descent for Federated Learning》被接收！
- ✉️ [2023] 向 AAAI 与 TPAMI 投稿最新成果

---

💼 **研究方向 / Research Interests**

- <strong>Federated Learning</strong>: 多客户端分布式建模、高效通信优化
- <strong>Efficient Fine-tuning</strong>: 参数微调方法（LoRA, BitFit, Adapter）
- <strong>Privacy-Preserving ML</strong>: 差分隐私机制与优化策略
- <strong>Optimization Theory</strong>: 动量方法、泛化误差、非凸理论

### 已发表论文

1. **FedBCGD: Communication-Efficient Accelerated Block Coordinate Gradient Descent for Federated Learning**  
   📌 ACM MM, 2024 · CCF A · 第一作者

2. **Improving Generalization in Federated Learning with Heterogeneous Data via Momentum-Based SWA**  
   📌 ICML, 2025 · CCF A · 第一作者

3. **Generalized Federated Learning via Sharpness Aware Minimization with Nesterov Extrapolation**  
   📌 ACM MM, 2025 · CCF A · 第一作者

4. **Temporal Dynamic Technology Convergence Prediction Based on DySAT Model**  
   📌 情报科学, 2023 · 中文核心 · 第二作者（学生一作）

---
### 已投或在审论文

1. **Dynamic Differentially Private Online ADMM Algorithms**  
   投稿至 IEEE TIFS · CCF A · SCI 一区（小修）  
   👤 学生一作，第二作者

2. **Tight High-Probability Bounds for Nonconvex Heavy-Tailed Scenario**  
   投稿至 NeurIPS · CCF A  
   👤 学生二作

3. **Finding Globally Flat Minima for DP Federated Learning**  
   投稿至 IEEE TIFS · CCF A · SCI 一区  
   👤 第一作者

4. **IGFL: Local & Global Consistency in Federated Learning**  
   投稿至 IEEE TPAMI · CCF A · SCI 一区  
   👤 第一作者

5. **FedAdamW: Convergence and Generalization in FL**  
   投稿至 AAAI · CCF A  
   👤 第一作者

6. **DP Federated Learning with Laplacian**  
   投稿至 IEEE TKDE · CCF A · SCI 一区  
   👤 学生一作


---
### 教育经历

- 📚 **博士（2025.06~）**  
  天津大学 · 计算机科学与技术

- 🎓 **硕士（2022.09 ~ 2025.06）**  
  西安电子科技大学 · 人工智能学院 · 计算机科学与技术

- 🎓 **本科（2018.09 ~ 2022.06）**  
  青岛大学 · 数学与应用数学


---

### 科研项目

**国自然面上项目：** 大规模机器学习动量加速方法的理论研究与应用  
⏱ 参与时间：2022.09 ~ 2024.12  
💼 职责：参与算法设计、实验搭建、论文撰写

### 获得荣誉

- 🎖 2024 年研究生国家奖学金  
- 🎖 2020 年本科生国家奖学金  
- 🎖 青岛大学年度十佳优秀学生  
- 🥈 全国大学生数学竞赛 全国二等奖  
- 🥈 全国大学生建模竞赛 全国二等奖  
- 🥇 美国大学生数学建模竞赛 M奖



如对我的研究方向感兴趣，欢迎邮件联系或学术合作！
